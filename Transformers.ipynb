{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85aa7416-be99-40bf-9785-2190aeaa598d",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "This task is split into 2 parts:\n",
    "\n",
    " - Some high level questions\n",
    " - Implement a decoder-only transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf47ee4-bd1d-4b58-8126-e4ed94cdebdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d84af9-e890-48b7-9fc3-36ec1aff07a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. What is different architecturally from the Transformer, vs a normal RNN, like an LSTM? (Specifically, how are recurrence and time managed?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614ef62-f917-4ac5-b21c-14ec36c482a6",
   "metadata": {},
   "source": [
    " - __RNN:__ At each time step the model combines one input vector (e.g. a word) with the previous state vector, to get an output. This can be run recurrently (lots of times) so that it takes e.g. one word from a sentence at a time.\n",
    " - __LSTM:__ A problem with RNNs is they suffer from decay of information, as the one state vector must compress information from all previous runs (each time with a weighting between the input vector and the previous state vector). LSTM's help solve this by having gates that let the model be more selective about which information enters the state vector, and which information it retreives on each run. This lets it store information for longer time spans.\n",
    " - __Transformer:__ Can also be run recurrently to generate a new series of tokens, but each time it's run it typically has inputs of all previous tokens (e.g. words). Then it uses the attention pattern (key-value queries on all tokens) to select the most relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44462c1c-2e62-4f58-ab81-aa266e6f8d03",
   "metadata": {},
   "source": [
    "### 2. Attention is defined as, $\\text{Attention}(Q,K,V) = \\text{softmax}(QK^T/\\sqrt{d_k})V$. What are the dimensions for $Q$, $K$, and $V$? Why do we use this setup? What other combinations could we do with $(Q,K)$ that also output weights?\n",
    "\n",
    "There are 2 key dimensions - $d_\\text{model}$ (which is the size of each layer) and $d_\\text{head}$ which is the size of each head output. Usually $d_\\text{model} = d_{head} \\times \\text{number of heads}$, as this way the attention sub-layer outputs the same number of activations as the input ($d_\\text{model}$) (which keeps things simple, although it doesn't have to be this way!).\n",
    "\n",
    "$$\n",
    "d_\\text{vocab} = \\text{Number of tokens (e.g. words)} \\\\\n",
    "d_\\text{head} = \\text{Dimension of each head} \\\\\n",
    "d_\\text{model} = \\text{Dimension of each model layer}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q \\in \\mathbb{R}^{d_\\text{vocab} \\times d_\\text{head}} \\\\\n",
    "K \\in \\mathbb{R}^{d_\\text{vocab} \\times d_\\text{head}} \\\\\n",
    "V \\in \\mathbb{R}^{d_\\text{vocab} \\times d_\\text{head}}\n",
    "$$\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    " 1. We start with $QK^T \\in \\mathbb{R}^{d_\\text{vocab} \\times d_\\text{vocab}}$ . \n",
    " \n",
    "The underlying calculation here is that for each head we want to dot-product multiply the query from that head by the key of every other head. This gives us a set of number scores which determin how much importance we should place on every other head's values). The result would be a vector of $\\text{1} \\times d_\\text{number of heads}$.\n",
    "\n",
    "Rather than doing one head at a time however, we do this for all heads "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
